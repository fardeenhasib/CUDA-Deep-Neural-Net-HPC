{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpqtrrtDm9Y6"
      },
      "source": [
        "# CUDA Neural Networks\n",
        "\n",
        "## Demo - Binary number classification with a CUDA Neural Network\n",
        "\n",
        "![alt text](https://i.imgur.com/FhGiI9R.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68EbUYm_nfOS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "eb9b0f0b-8817-4f93-91a5-499b9562856d"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true simple.cu -o simple -lcudadevrt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[01m\u001b[Kgcc:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[Ksimple.cu: No such file or directory\n",
            "\u001b[01m\u001b[Kgcc:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[K-x c++\u001b[m\u001b[K’ after last input file has no effect\n",
            "\u001b[01m\u001b[Kgcc:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kno input files\n",
            "compilation terminated.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c8SOMNSns81",
        "outputId": "506a745c-a440-43fe-91ed-48b4b683c4ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "!./simple"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction[0] : 0.060997 True Value[0] : 0.000000 Error[0] : 0.060997\n",
            "Prediction[1] : 0.076193 True Value[1] : 0.000000 Error[1] : 0.076193\n",
            "Prediction[2] : 0.927551 True Value[2] : 1.000000 Error[2] : -0.072449\n",
            "Prediction[3] : 0.918263 True Value[3] : 1.000000 Error[3] : -0.081737\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H5cJHqboO_S"
      },
      "source": [
        "#include <omp.h>\n",
        "#include <bits/stdc++.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "#include <time.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "#define MAX_MATRIX_SIZE 65536\n",
        "#define DEBUG false\n",
        "#define BLOCK_SIZE 16\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void kMartixByMatrixElementwise(const int nThreads, const float *m1, const float *m2, float *output) {\n",
        "    /*  Computes the product of two arrays (elementwise multiplication).\n",
        "     Inputs:\n",
        "     m1: array\n",
        "     m2: array\n",
        "     output: array,the results of the multiplication are to be stored here\n",
        "    */\n",
        "\tfor (int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\t i < nThreads;\n",
        "\t\t i += blockDim.x * gridDim.x)\n",
        "\t  {\n",
        "\t\toutput[i] = m1[i] * m2[i];\n",
        "\t  }\n",
        "}\n",
        "\n",
        "__device__ float* dMartixByMatrixElementwise(const float *m1, const float *m2, float *output, const int width, const int height){\n",
        "\n",
        "\tkMartixByMatrixElementwise <<< width, height >>> ( width * height, m1, m2, output );\n",
        "    cudaDeviceSynchronize();\n",
        "    return output;\n",
        "}\n",
        "\n",
        "__global__ void kMartixSubstractMatrix(const int nThreads, const float *m1, const float *m2, float *output) {\n",
        "    /*  Computes the (elementwise) difference between two arrays\n",
        "     Inputs:\n",
        "     m1: array\n",
        "     m2: array\n",
        "     output: array,the results of the computation are to be stored here\n",
        "     */\n",
        "\n",
        "\tfor (int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\t i < nThreads;\n",
        "\t\t i += blockDim.x * gridDim.x)\n",
        "\t  {\n",
        "\t\toutput[i] = m1[i] - m2[i];\n",
        "\t  }\n",
        "}\n",
        "\n",
        "__device__ float* dMartixSubstractMatrix(const float *m1, const float *m2, float *output, const int width, const int height){\n",
        "\n",
        "\tkMartixSubstractMatrix <<< width, height >>> ( width * height, m1, m2, output );\n",
        "    cudaDeviceSynchronize();\n",
        "    return output;\n",
        "}\n",
        "\n",
        "__global__ void kSigmoid(const int nThreads, float const *input, float *output){\n",
        "    /*  Computes the value of the sigmoid function f(x) = 1/(1 + e^-x).\n",
        "     Inputs:\n",
        "     input: array\n",
        "     output: array, the results of the computation are to be stored here\n",
        "    */\n",
        "\n",
        "\tfor (int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\t i < nThreads;\n",
        "\t\t i += blockDim.x * gridDim.x)\n",
        "\t  {\n",
        "\t\toutput[i] = 1.0 / (1.0 + std::exp(-input[i]));\n",
        "\t  }\n",
        "}\n",
        "\n",
        "__device__ void dSigmoid(float const *input, float *output, const int height, const int width){\n",
        "\n",
        "\tkSigmoid <<< height, width >>> (height * width, input, output);\n",
        "\tcudaDeviceSynchronize();\n",
        "}\n",
        "\n",
        "__global__ void kSigmoid_d(const int nThreads, float const *input, float *output) {\n",
        "\t/*  Computes the value of the sigmoid function derivative f'(x) = f(x)(1 - f(x)),\n",
        "\t    where f(x) is sigmoid function.\n",
        "\t    Inputs:\n",
        "\t    input: array\n",
        "\t    output: array, the results of the computation are to be stored here:\n",
        "\t    \t\tx(1 - x) for every element of the input matrix m1.\n",
        "\t*/\n",
        "\n",
        "\tfor (int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\t i < nThreads;\n",
        "\t\t i += blockDim.x * gridDim.x)\n",
        "\t  {\n",
        "\t\toutput[i] = input[i] * (1 - input[i]);\n",
        "\t  }\n",
        "}\n",
        "\n",
        "__device__ float* dSigmoid_d(float const *input, float *output, const int rows, const int columns){\n",
        "\tkSigmoid_d <<< rows, columns >>> (rows*columns, input, output);\n",
        "\tcudaDeviceSynchronize();\n",
        "\treturn output;\n",
        "}\n",
        "\n",
        "__global__ void kDot(const int nThreads, const float *m1, const float *m2, float *output, const int m1_rows , const int m1_columns, const int m2_columns ){\n",
        "\t/*  Computes the product of two matrices: m1 x m2.\n",
        "\t   \tInputs:\n",
        "\t    m1: array, left matrix of size m1_rows x m1_columns\n",
        "\t    m2: array, right matrix of size m1_columns x m2_columns (the number of rows in the right matrix\n",
        "\t    must be equal to the number of the columns in the left one)\n",
        "\t    output: array, the results of the computation are to be stored here:\n",
        "\t    \t\tm1 * m2, product of two arrays m1 and m2, a matrix of size m1_rows x m2_columns\n",
        "\t    m1_rows: int, number of rows in the left matrix m1\n",
        "\t    m1_columns: int, number of columns in the left matrix m1\n",
        "\t    m2_columns: int, number of columns in the right matrix m2\n",
        "\t*/\n",
        "\n",
        "\tfor (int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\t i < nThreads;\n",
        "\t\t i += blockDim.x * gridDim.x)\n",
        "\t{\n",
        "\t    int r = (int)i / m2_columns;\n",
        "\t    int c = i % m2_columns;\n",
        "\t    float t_output = 0.f;\n",
        "\n",
        "\t    for( int k = 0; k < m1_columns; ++k ) {\n",
        "\t        t_output += m1[ r * m1_columns + k ] * m2[ k * m2_columns + c ];\n",
        "\t    }\n",
        "\n",
        "\t    output[i] = t_output;\n",
        "\t}\n",
        "}\n",
        "\n",
        "__device__ float* dDot(const float *m1, const float *m2, float *output, const int m1_rows , const int m1_columns, const int m2_columns ){\n",
        "\n",
        "\tkDot <<< m1_rows, m2_columns >>> (m1_rows * m2_columns, m1, m2, output, m1_rows , m1_columns, m2_columns );\n",
        "\tcudaDeviceSynchronize();\n",
        "\treturn output;\n",
        "}\n",
        "\n",
        "__global__ void kDot_m1_m2T(const int nThreads, const float *m1, const float *m2, float *output, const int m1_columns, const int m2_rows ){\n",
        "\t/*  Updates the output matrix with the product of two matrices: m1 and m2 transposed.\n",
        "\t   \tInputs:\n",
        "\t    m1: array, left matrix of size m1_rows x m1_columns\n",
        "\t    m2: array, right matrix of size m2_rows x m1_columns (m2 transposed will be of size m1_columns x m2_rows)\n",
        "\t    output: array, the results of the computation are to be stored here:\n",
        "\t    \t\tm1 * m2, product of two arrays m1 and m2, a matrix of size m1_rows x m2_rows\n",
        "\t    m1_columns: int, number of columns in the left matrix m1\n",
        "\t    m2_rows: int, number of rows in the left matrix m2\n",
        "\t*/\n",
        "\n",
        "\tfor (int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\t i < nThreads;\n",
        "\t\t i += blockDim.x * gridDim.x)\n",
        "\t{\n",
        "\t\tint r = (int)i / m2_rows;\n",
        "\t\tint c = i % m2_rows;\n",
        "\t\tfloat t_output = 0.0;\n",
        "\t\tint id_T;\n",
        "\n",
        "\t\tfor( int k = 0; k < m1_columns; ++k ) {\n",
        "\t\t\tid_T = c * m1_columns + k;\n",
        "\t\t\tt_output += m1[ r * m1_columns + k ] * m2[ id_T ];\n",
        "\t\t}\n",
        "\n",
        "\t\toutput[i] = t_output;\n",
        "\t}\n",
        "}\n",
        "\n",
        "__device__ float* dDot_m1_m2T(const float *m1, const float *m2, float *output, const int m1_rows , const int m1_columns, const int m2_rows )\n",
        "{\n",
        "\tkDot_m1_m2T <<< m1_rows, m2_rows >>> ( m1_rows * m2_rows, m1, m2, output, m1_columns, m2_rows );\n",
        "\tcudaDeviceSynchronize();\n",
        "\treturn output;\n",
        "}\n",
        "\n",
        "__global__ void kDot_m1T_m2(const int nThreads, const float *m1, const float *m2, float *output, const int m1_rows,\n",
        "\t\t\t\t\t\t\tconst int m1_columns, const int m2_columns ){\n",
        "\t/*  Increments the output matrix with the product of two matrices: m1 transposed and m2.\n",
        "\t   \tInputs:\n",
        "\t    m1: array, left matrix of size m1_rows x m1_columns (m1 transposed will be of size m1_columns x m1_rows)\n",
        "\t    m2: array, right matrix of size m1_rows x m2_columns\n",
        "\t    output: array, the results of the computation are to be stored here:\n",
        "\t    \t\tm1 * m2, product of two arrays m1 and m2, a matrix of size m1_columns x m2_columns\n",
        "\t    m1_rows: int, number of rows in the left matrix m1\n",
        "\t    m1_columns: int, number of columns in the left matrix m1\n",
        "\t    m2_rows: int, number of rows in the left matrix m2\n",
        "\t*/\n",
        "\n",
        "\tfor (int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\t i < nThreads;\n",
        "\t\t i += blockDim.x * gridDim.x)\n",
        "\t{\n",
        "\t    int r = (int)i / m2_columns;\n",
        "\t    int c = i % m2_columns;\n",
        "\t    int id_T;\n",
        "\t    float t_output = 0.0;\n",
        "\n",
        "\t    for( int k = 0; k < m1_rows; ++k ) {\n",
        "\t    \tid_T = k * m1_columns + r;\n",
        "\t        t_output += m1[ id_T ] * m2[ k * m2_columns + c ];\n",
        "\t    }\n",
        "\n",
        "\t    output[i] += t_output;\n",
        "\t}\n",
        "}\n",
        "\n",
        "__device__ void dDot_m1T_m2(const float *m1, const float *m2, float *output, const int m1_height , const int m1_width, const int m2_width )\n",
        "{\n",
        "\tkDot_m1T_m2 <<< m1_width, m2_width >>> (m1_width * m2_width, m1, m2, output, m1_height, m1_width, m2_width );\n",
        "\tcudaDeviceSynchronize();\n",
        "}\n",
        "\n",
        "__device__ void kPrintMatrix (const float* M, int h, int w) {\n",
        "    /*  Prints out the input array as h x w matrix.\n",
        "     Inputs:\n",
        "     m: vector, matrix of size n_rows x n_columns\n",
        "     h: int, number of rows in the matrix M\n",
        "     w: int, number of columns in the matrix M\n",
        "     */\n",
        "\tfor (int i = 0; i < h; i++){\n",
        "\t\tfor (int j = 0; j < w; j++){\n",
        "\t\t\tprintf(\"%f  \", M[i*w+j]);\n",
        "\t\t}\n",
        "\t\tprintf(\"\\n\");\n",
        "\t}\n",
        "\tprintf(\"\\n\");\n",
        "}\n",
        "\n",
        "__global__ void kFit(\tconst float* X, const int X_w, const int X_h,\n",
        "\t\t\t\t\t\tconst float* y, const int y_w,\n",
        "\t\t\t\t\t\tfloat* l1, const int l1_w, float* l_1_d,\n",
        "\t\t\t\t\t\tfloat* pred, float* pred_d,\n",
        "\t\t\t\t\t\tfloat* W0,\n",
        "\t\t\t\t\t\tfloat* W1,\n",
        "\t\t\t\t\t\tfloat* buffer\n",
        "\t\t\t\t\t\t)\n",
        "{\n",
        "\tfor (unsigned i = 0; i < 50; ++i) {\n",
        "\n",
        "        dSigmoid(dDot(X, W0, l1, X_h, X_w, l1_w), l1, X_h, l1_w);\n",
        "        dSigmoid(dDot(l1, W1, pred, X_h, l1_w, y_w), pred, X_h, y_w);\n",
        "        dMartixByMatrixElementwise(dMartixSubstractMatrix(y, pred, pred_d, X_h, y_w), dSigmoid_d(pred, buffer, X_h, y_w), pred_d, X_h, y_w );\n",
        "        dMartixByMatrixElementwise(dDot_m1_m2T(pred_d, W1, l_1_d, X_h, y_w, l1_w), dSigmoid_d(l1, buffer, X_h, l1_w), l_1_d, X_h, l1_w);\n",
        "        dDot_m1T_m2( l1, pred_d, W1, X_h, l1_w, y_w );\n",
        "        dDot_m1T_m2( X, l_1_d, W0, X_h, X_w, l1_w );\n",
        "    }\n",
        "}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Strassen Algorithm implementation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "int matrix_size, terminal_matrix_size;\n",
        "\n",
        "\n",
        "\n",
        "void print(int n, int** mat)\n",
        "{\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        for (int j = 0; j < n; j++) {\n",
        "            cout << mat[i][j] << \" \";\n",
        "        }\n",
        "        cout << endl;\n",
        "    }\n",
        "    cout << endl;\n",
        "}\n",
        "\n",
        "\n",
        "int** getSlice(int n, int** mat, int offseti, int offsetj)\n",
        "{\n",
        "    int m = n / 2;\n",
        "\n",
        "    int** slice = (int**)malloc(n * sizeof(int*));\n",
        "    int* data_slice = (int*)malloc(n * n * sizeof(int));\n",
        "\n",
        "    for (int i = 0; i < n; i++)\n",
        "    {\n",
        "        slice[i] = &(data_slice[n * i]);\n",
        "    }\n",
        "\n",
        "    for (int i = 0; i < m; i++)\n",
        "    {\n",
        "        for (int j = 0; j < m; j++)\n",
        "        {\n",
        "            slice[i][j] = mat[offseti + i][offsetj + j];\n",
        "        }\n",
        "    }\n",
        "    return slice;\n",
        "}\n",
        "\n",
        "int** addMatrices(int n, int** mat1, int** mat2, bool add)\n",
        "{\n",
        "    int** result = (int**)malloc(n * sizeof(int*));\n",
        "    int* data_result = (int*)malloc(n * n * sizeof(int));\n",
        "\n",
        "    for (int i = 0; i < n; i++)\n",
        "    {\n",
        "        result[i] = &(data_result[n * i]);\n",
        "    }\n",
        "\n",
        "    for (int i = 0; i < n; i++)\n",
        "    {\n",
        "        for (int j = 0; j < n; j++)\n",
        "        {\n",
        "            if (add)\n",
        "                result[i][j] = mat1[i][j] + mat2[i][j];\n",
        "            else\n",
        "                result[i][j] = mat1[i][j] - mat2[i][j];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return result;\n",
        "}\n",
        "\n",
        "int** combineMatrices(int m, int** c11, int** c12, int** c21, int** c22)\n",
        "{\n",
        "    int n = 2 * m;\n",
        "\n",
        "    int** result = (int**)malloc(n * sizeof(int*));\n",
        "    int* data_result = (int*)malloc(n * n * sizeof(int));\n",
        "\n",
        "    for (int i = 0; i < n; i++)\n",
        "    {\n",
        "        result[i] = &(data_result[n * i]);\n",
        "    }\n",
        "\n",
        "    for (int i = 0; i < n; i++)\n",
        "    {\n",
        "        for (int j = 0; j < n; j++)\n",
        "        {\n",
        "            if (i < m && j < m)\n",
        "                result[i][j] = c11[i][j];\n",
        "            else if (i < m)\n",
        "                result[i][j] = c12[i][j - m];\n",
        "            else if (j < m)\n",
        "                result[i][j] = c21[i - m][j];\n",
        "            else\n",
        "                result[i][j] = c22[i - m][j - m];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return result;\n",
        "}\n",
        "\n",
        "\n",
        "int** naive(int n, int** mat1, int** mat2)\n",
        "{\n",
        "    int** prod = (int**)malloc(n * sizeof(int*));\n",
        "    int* data_prod = (int*)malloc(n * n * sizeof(int));\n",
        "\n",
        "    for (int i = 0; i < n; i++)\n",
        "    {\n",
        "        prod[i] = &(data_prod[n * i]);\n",
        "    }\n",
        "\n",
        "    for (int i = 0; i < n; i++)\n",
        "    {\n",
        "        for (int j = 0; j < n; j++)\n",
        "        {\n",
        "            prod[i][j] = 0;\n",
        "            for (int k = 0; k < n; k++)\n",
        "            {\n",
        "                prod[i][j] += mat1[i][k] * mat2[k][j];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return prod;\n",
        "}\n",
        "\n",
        "__global__ void multiply(int *left, int *right, int *res, int dim) {\n",
        "\n",
        "    int i,j;\n",
        "    int temp = 0;\n",
        "\n",
        "    __shared__ float Left_shared_t [BLOCK_SIZE][BLOCK_SIZE];\n",
        "    __shared__ float Right_shared_t[BLOCK_SIZE][BLOCK_SIZE];\n",
        "\n",
        "    // Row i of matrix left\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "\n",
        "    for (int tileNUM = 0; tileNUM < gridDim.x; tileNUM++) {\n",
        "\n",
        "        // Column j of matrix left\n",
        "        j = tileNUM * BLOCK_SIZE + threadIdx.x;\n",
        "        i = tileNUM * BLOCK_SIZE + threadIdx.y;\n",
        "        // Load left[i][j] to shared mem\n",
        "\n",
        "        Left_shared_t[threadIdx.y][threadIdx.x] = left[row * dim + j];// Coalesced access\n",
        "        // Load right[i][j] to shared mem\n",
        "\n",
        "        Right_shared_t[threadIdx.y][threadIdx.x] = right[i * dim + col]; // Coalesced access\n",
        "        // Synchronize before computation\n",
        "        __syncthreads();\n",
        "\n",
        "        // Accumulate one tile of res from tiles of left and right in shared mem\n",
        "        for (int k = 0; k < BLOCK_SIZE; k++) {\n",
        "\n",
        "            temp += Left_shared_t[threadIdx.y][k] * Right_shared_t[k][threadIdx.x]; //no shared memory bank conflict\n",
        "        }\n",
        "        // Synchronize\n",
        "        __syncthreads();\n",
        "    }\n",
        "    // Store accumulated value to res\n",
        "    res[row * dim + col] = temp;\n",
        "}\n",
        "\n",
        "int** cudaNaive(int n, int** mat1, int** mat2)\n",
        "{\n",
        "    size_t bytes;\n",
        "    bytes = n*n * sizeof(int);\n",
        "\n",
        "    int* h_mat1 = (int*)malloc(bytes);\n",
        "\n",
        "    for(int i=0;i<n;i++){\n",
        "        for(int j=0;j<n;j++){\n",
        "            h_mat1[i*n + j] = mat1[i][j];\n",
        "            //cout<< h_mat1[i*n +j]<<\" \";\n",
        "        }\n",
        "        //cout<<endl;\n",
        "    }\n",
        "\n",
        "    int* h_mat2 = (int*)malloc(bytes);\n",
        "    for(int i=0;i<n;i++){\n",
        "        for(int j=0;j<n;j++){\n",
        "            h_mat2[i*n + j] = mat2[i][j];\n",
        "           //cout<< h_mat2[i*n +j]<<\" \";\n",
        "        }\n",
        "        //cout<<endl;\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "    int* h_product = (int*)malloc(bytes);\n",
        "    int *d_mat1, *d_mat2, *d_product;\n",
        "\n",
        "    cudaMalloc((void**)&d_mat1, bytes);\n",
        "    cudaMalloc((void**)&d_mat2, bytes);\n",
        "    cudaMalloc((void**)&d_product, bytes);\n",
        "\n",
        "    cudaMemcpy(d_mat1, h_mat1, bytes, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_mat2, h_mat2, bytes, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_product, h_product, bytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "    int block_size = min(n, 16);\n",
        "    //dim3 Block_dim(BLOCK_SIZE, BLOCK_SIZE);\n",
        "    dim3 Block_dim(block_size, block_size);\n",
        "    //Grid dimension is found by dividing matrix dimension to block_size\n",
        "    dim3 Grid_dim(n / block_size, n / block_size);\n",
        "\n",
        "    multiply<<<Grid_dim, Block_dim>>>(d_mat1, d_mat2, d_product, n);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    cudaMemcpy(h_product, d_product, bytes, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    int** product = (int**)malloc(n * sizeof(int*));\n",
        "    int* data_product = (int*)malloc(n * n * sizeof(int));\n",
        "    for (int i = 0; i < n; i++)\n",
        "    {\n",
        "        product[i] = &(data_product[n * i]);\n",
        "    }\n",
        "\n",
        "    for(int i=0;i<n;i++){\n",
        "        for(int j=0;j<n;j++){\n",
        "            // cout<< h_product[i*n+j]<<\" \";\n",
        "            product[i][j] = h_product[i*n + j];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    cudaFree(d_mat1);\n",
        "    cudaFree(d_mat2);\n",
        "    cudaFree(d_product);\n",
        "\n",
        "    free(h_mat1);\n",
        "    free(h_mat2);\n",
        "\n",
        "    return product;\n",
        "}\n",
        "\n",
        "\n",
        "int** strassen(int n, int** mat1, int** mat2)\n",
        "{\n",
        "\n",
        "    if (n <= terminal_matrix_size)\n",
        "    {\n",
        "        return cudaNaive(n, mat1, mat2);\n",
        "    }\n",
        "\n",
        "    int m = n / 2;\n",
        "\n",
        "    int** a = getSlice(n, mat1, 0, 0);\n",
        "    int** b = getSlice(n, mat1, 0, m);\n",
        "    int** c = getSlice(n, mat1, m, 0);\n",
        "    int** d = getSlice(n, mat1, m, m);\n",
        "    int** e = getSlice(n, mat2, 0, 0);\n",
        "    int** f = getSlice(n, mat2, 0, m);\n",
        "    int** g = getSlice(n, mat2, m, 0);\n",
        "    int** h = getSlice(n, mat2, m, m);\n",
        "\n",
        "    int** bds = addMatrices(m, b, d, false);\n",
        "    int** gha = addMatrices(m, g, h, true);\n",
        "    int** s1 = strassen(m, bds, gha);\n",
        "\n",
        "    free(bds[0]); free(bds);\n",
        "\n",
        "    free(gha[0]); free(gha);\n",
        "\n",
        "    int** ada = addMatrices(m, a, d, true);\n",
        "    int** eha = addMatrices(m, e, h, true);\n",
        "    int** s2 = strassen(m, ada, eha);\n",
        "\n",
        "    free(ada[0]); free(ada);\n",
        "\n",
        "    free(eha[0]); free(eha);\n",
        "\n",
        "    int** acs = addMatrices(m, a, c, false);\n",
        "    int** efa = addMatrices(m, e, f, true);\n",
        "    int** s3 = strassen(m, acs, efa);\n",
        "\n",
        "    free(acs[0]); free(acs);\n",
        "\n",
        "    free(efa[0]); free(efa);\n",
        "\n",
        "    int** aba = addMatrices(m, a, b, true);\n",
        "    int** s4 = strassen(m, aba, h);\n",
        "\n",
        "    free(aba[0]); free(aba);\n",
        "    free(b[0]); free(b);\n",
        "\n",
        "    int** fhs = addMatrices(m, f, h, false);\n",
        "    int** s5 = strassen(m, a, fhs);\n",
        "\n",
        "    free(fhs[0]); free(fhs);\n",
        "    free(a[0]); free(a);\n",
        "    free(f[0]); free(f);\n",
        "    free(h[0]); free(h);\n",
        "\n",
        "    int** ges = addMatrices(m, g, e, false);\n",
        "    int** s6 = strassen(m, d, ges);\n",
        "\n",
        "    free(ges[0]); free(ges);\n",
        "    free(g[0]); free(g);\n",
        "\n",
        "    int** cda = addMatrices(m, c, d, true);\n",
        "    int** s7 = strassen(m, cda, e);\n",
        "\n",
        "    free(cda[0]); free(cda);\n",
        "    free(c[0]); free(c);\n",
        "    free(d[0]); free(d);\n",
        "    free(e[0]); free(e);\n",
        "\n",
        "    int** s1s2a = addMatrices(m, s1, s2, true);\n",
        "    int** s6s4s = addMatrices(m, s6, s4, false);\n",
        "    int** c11 = addMatrices(m, s1s2a, s6s4s, true);\n",
        "\n",
        "    free(s1s2a[0]); free(s1s2a);\n",
        "    free(s6s4s[0]); free(s6s4s);\n",
        "    free(s1[0]); free(s1);\n",
        "\n",
        "    int** c12 = addMatrices(m, s4, s5, true);\n",
        "    free(s4[0]); free(s4);\n",
        "\n",
        "    int** c21 = addMatrices(m, s6, s7, true);\n",
        "    free(s6[0]); free(s6);\n",
        "\n",
        "    int** s2s3s = addMatrices(m, s2, s3, false);\n",
        "    int** s5s7s = addMatrices(m, s5, s7, false);\n",
        "    int** c22 = addMatrices(m, s2s3s, s5s7s, true);\n",
        "\n",
        "    free(s2s3s[0]); free(s2s3s);\n",
        "    free(s5s7s[0]); free(s5s7s);\n",
        "    free(s2[0]); free(s2);\n",
        "    free(s3[0]); free(s3);\n",
        "    free(s5[0]); free(s5);\n",
        "    free(s7[0]); free(s7);\n",
        "\n",
        "    int** prod = combineMatrices(m, c11, c12, c21, c22);\n",
        "\n",
        "    free(c11[0]); free(c11);\n",
        "    free(c12[0]); free(c12);\n",
        "    free(c21[0]); free(c21);\n",
        "    free(c22[0]); free(c22);\n",
        "\n",
        "    return prod;\n",
        "}"
      ],
      "metadata": {
        "id": "XoyFpcY8wkxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "__global__ void copy_row(int * mat, int * transpose, int nx, int ny)\n",
        "{\n",
        "\tint ix = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\tint iy = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "\tif (ix < nx && iy < ny)\n",
        "\t{\n",
        "\t\ttranspose[iy * nx + ix] = mat[iy * nx + ix];\n",
        "\t}\n",
        "}\n",
        "\n",
        "__global__ void copy_column(int * mat, int * transpose, int nx, int ny)\n",
        "{\n",
        "\tint ix = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\tint iy = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "\tif (ix < nx && iy < ny)\n",
        "\t{\n",
        "\t\ttranspose[ix * ny + iy] = mat[ix * ny + iy];\n",
        "\t}\n",
        "}\n",
        "\n",
        "__global__ void transpose_read_row_write_column(int * mat, int * transpose, int nx, int ny)\n",
        "{\n",
        "\tint ix = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\tint iy = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "\tif (ix < nx && iy < ny)\n",
        "\t{\n",
        "\t\ttranspose[ix * ny + iy] = mat[iy * nx + ix];\n",
        "\t}\n",
        "}\n",
        "\n",
        "__global__ void transpose_read_column_write_row(int * mat, int * transpose, int nx, int ny)\n",
        "{\n",
        "\tint ix = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\tint iy = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "\tif (ix < nx && iy < ny)\n",
        "\t{\n",
        "\t\ttranspose[iy * nx + ix] = mat[ix * ny + iy];\n",
        "\t}\n",
        "}\n",
        "\n",
        "__global__ void transpose_unroll4_row(int * mat, int * transpose, int nx, int ny)\n",
        "{\n",
        "\tint ix = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n",
        "\tint iy = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "\tint ti = iy * nx + ix;\n",
        "\tint to = ix * ny + iy;\n",
        "\n",
        "\tif (ix + 3 * blockDim.x < nx && iy < ny)\n",
        "\t{\n",
        "\t\ttranspose[to]\t\t\t\t\t\t= mat[ti];\n",
        "\t\ttranspose[to + ny*blockDim.x]\t\t= mat[ti + blockDim.x];\n",
        "\t\ttranspose[to + ny * 2 * blockDim.x] = mat[ti + 2 * blockDim.x];\n",
        "\t\ttranspose[to + ny * 3 * blockDim.x] = mat[ti + 3 * blockDim.x];\n",
        "\t}\n",
        "}\n",
        "\n",
        "__global__ void transpose_unroll4_col(int * mat, int * transpose, int nx, int ny)\n",
        "{\n",
        "\tint ix = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n",
        "\tint iy = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "\tint ti = iy * nx + ix;\n",
        "\tint to = ix * ny + iy;\n",
        "\n",
        "\tif (ix + 3 * blockDim.x < nx && iy < ny)\n",
        "\t{\n",
        "\t\ttranspose[ti] = mat[to];\n",
        "\t\ttranspose[ti + blockDim.x] = mat[to + blockDim.x*ny];\n",
        "\t\ttranspose[ti + 2 * blockDim.x] = mat[to + 2 * blockDim.x*ny];\n",
        "\t\ttranspose[ti + 3 * blockDim.x] = mat[to + 3 * blockDim.x*ny];\n",
        "\t}\n",
        "}\n",
        "\n",
        "__global__ void transpose_diagonal_row(int * mat, int * transpose, int nx, int ny)\n",
        "{\n",
        "\tint blk_x = blockIdx.x;\n",
        "\tint blk_y = (blockIdx.x + blockIdx.y) % gridDim.x;\n",
        "\n",
        "\tint ix = blockIdx.x * blk_x + threadIdx.x;\n",
        "\tint iy = blockIdx.y * blk_y + threadIdx.y;\n",
        "\n",
        "\tif (ix < nx && iy < ny)\n",
        "\t{\n",
        "\t\ttranspose[ix * ny + iy] = mat[iy * nx + ix];\n",
        "\t}\n",
        "}\n"
      ],
      "metadata": {
        "id": "mn5xUgIgybnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int main(void){\n",
        "\n",
        "\tconst int TRAINING_SIZE = 4;\n",
        "\tconst int TRAINING_DIM = 4;\n",
        "\tconst int L1_SIZE = 8;\n",
        "\n",
        "\t// X, the first 4 lines from Iris dataset\n",
        "\tfloat h_X[TRAINING_SIZE*TRAINING_DIM] = {\t5.1, 3.5, 1.4, 0.2,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t4.9, 3.0, 1.4, 0.2,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t6.2, 3.4, 5.4, 2.3,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t5.9, 3.0, 5.1, 1.8 };\n",
        "\n",
        "\tconst signed int X_size = sizeof(h_X);\n",
        "\n",
        "\tfloat *d_X;\n",
        "\tcudaMalloc(&d_X, X_size);\n",
        "\tcudaMemcpy(d_X, h_X, X_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\t//WEIGHTS_0\n",
        "\tconst long signed int W0_size = L1_SIZE*TRAINING_DIM*sizeof(float);\n",
        "\tfloat *h_W0 = (float*)malloc(W0_size);\n",
        "\tfor (int i = 0; i < L1_SIZE*TRAINING_DIM; i++){\n",
        "\t    h_W0[i] = 0.1 * (2.0*rand()/RAND_MAX-1.0);\n",
        "\t}\n",
        "\n",
        "\tfloat *d_W0;\n",
        "\tcudaMalloc(&d_W0, W0_size);\n",
        "\tcudaMemcpy(d_W0, h_W0, W0_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\t//LAYER_1, LAYER_1_DELTA AND BUFFER OF LAYER 1 SIZE\n",
        "\tconst long signed int L1_size = L1_SIZE*TRAINING_SIZE*sizeof(float);\n",
        "\n",
        "\tfloat* h_layer_1 = (float*)malloc(L1_size);\n",
        "\tfloat* h_layer_1_delta = (float*)malloc(L1_size);\n",
        "\tfloat* h_buffer = (float*)malloc(L1_size);\n",
        "\n",
        "\tfor (int i = 0; i < L1_SIZE*TRAINING_SIZE; i++){\n",
        "\t    h_layer_1[i] = 0.0;\n",
        "\t    h_buffer[i] = 0.0;\n",
        "\t    h_layer_1_delta[i] = 0.0;\n",
        "\t}\n",
        "\n",
        "\tfloat *d_layer_1;\n",
        "\tcudaMalloc(&d_layer_1, L1_size);\n",
        "\tcudaMemcpy(d_layer_1, h_layer_1, L1_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\tfloat *d_buffer;\n",
        "\tcudaMalloc(&d_buffer, L1_size);\n",
        "\tcudaMemcpy(d_buffer, h_buffer, L1_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\tfloat *d_layer_1_delta;\n",
        "\tcudaMalloc(&d_layer_1_delta, L1_size);\n",
        "\tcudaMemcpy(d_layer_1_delta, h_layer_1_delta, L1_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\t//WEIGHTS_1\n",
        "\tconst long signed int W1_size = L1_SIZE*sizeof(float);\n",
        "\tfloat *h_W1 = (float*)malloc(W1_size);\n",
        "\tfor (int i = 0; i < L1_SIZE; i++){\n",
        "\t    h_W1[i] = 0.1* (2.0*rand()/RAND_MAX-1.0);\n",
        "\t}\n",
        "\n",
        "\tfloat *d_W1;\n",
        "\tcudaMalloc(&d_W1, W1_size);\n",
        "\tcudaMemcpy(d_W1, h_W1, W1_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\t//Y\n",
        "\tfloat h_y[4] = {\t0,\n",
        "\t\t\t\t\t\t0,\n",
        "\t\t\t\t\t\t1,\n",
        "\t\t\t\t\t\t1 };\n",
        "\tconst signed int y_size = sizeof(h_y);\n",
        "\tfloat *d_y;\n",
        "\tcudaMalloc(&d_y, y_size);\n",
        "\tcudaMemcpy(d_y, h_y, y_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\t//PRED AND PRED_DELTA\n",
        "\tfloat* h_pred = (float*)malloc(y_size);\n",
        "\tfloat* h_pred_delta = (float*)malloc(y_size);\n",
        "\tfor (int i = 0; i < TRAINING_SIZE; i++){\n",
        "\t    h_pred[i] = 0.0;\n",
        "\t    h_pred_delta[i] = 0.0;\n",
        "\t}\n",
        "\n",
        "\tfloat *d_pred;\n",
        "\tcudaMalloc(&d_pred, y_size);\n",
        "\tcudaMemcpy(d_pred, h_pred, y_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\tfloat *d_pred_delta;\n",
        "\tcudaMalloc(&d_pred_delta, y_size);\n",
        "\tcudaMemcpy(d_pred_delta, h_pred_delta, y_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\tkFit <<< 1, 1 >>> (\td_X, TRAINING_DIM, TRAINING_SIZE,\n",
        "\t\t\t\t\t\td_y, 1,\n",
        "\t\t\t\t\t\td_layer_1, L1_SIZE, d_layer_1_delta,\n",
        "\t\t\t\t\t\td_pred,\n",
        "\t\t\t\t\t\td_pred_delta,\n",
        "\t\t\t\t\t\td_W0,\n",
        "\t\t\t\t\t\td_W1,\n",
        "\t\t\t\t\t\td_buffer);\n",
        "\n",
        "\tcudaMemcpy(h_pred, d_pred, y_size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "\tcudaFree(d_pred);\n",
        "\tcudaFree(d_X);\n",
        "\tcudaFree(d_y);\n",
        "\tcudaFree(d_layer_1_delta);\n",
        "\tcudaFree(d_pred_delta);\n",
        "\tcudaFree(d_W0);\n",
        "\tcudaFree(d_W1);\n",
        "\tcudaFree(d_buffer);\n",
        "\n",
        "\tfree(h_layer_1_delta);\n",
        "\tfree(h_pred_delta);\n",
        "\tfree(h_W0);\n",
        "\tfree(h_W1);\n",
        "\tfree(h_buffer);\n",
        "\n",
        "\tfor (int i = 0; i < TRAINING_SIZE; i++){\n",
        "\t\tprintf(\"Prediction[%i] : %f True Value[%i] : %f Error[%i] : %f\\n\", i, h_pred[i], i, h_y[i], i, h_pred[i] - h_y[i]);\n",
        "\t}\n",
        "\n",
        "\tfree(h_pred);\n",
        "}"
      ],
      "metadata": {
        "id": "HhzhFEDtwUnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoxIGcAfoFBN"
      },
      "source": [
        "## Why Learn CUDA?\n",
        "Have a look https://github.com/pytorch/pytorch/tree/5bc44fb6ea65c711c818ea82dc66afee9ad48f78/aten/src/ATen/native/cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie3pMAGUsqaM"
      },
      "source": [
        "## Install CUDA in Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvvigQF_spPG"
      },
      "source": [
        "!apt update -qq;\n",
        "!wget https://developer.nvidia.com/compute/cuda/8.0/Prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb;\n",
        "!dpkg -i cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb;\n",
        "!apt-key add /var/cuda-repo-8-0-local-ga2/7fa2af80.pub;\n",
        "!apt-get update -qq;\n",
        "!apt-get install cuda gcc-5 g++-5 -y -qq;\n",
        "!ln -s /usr/bin/gcc-5 /usr/local/cuda/bin/gcc;\n",
        "!ln -s /usr/bin/g++-5 /usr/local/cuda/bin/g++;\n",
        "!apt install cuda-8.0;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef0oNYUWstr4",
        "outputId": "ea6204a6-bd29-4b00-f14c-5b055bbddf15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin\n",
        "\n",
        "\n",
        "%%cu\n",
        "#include <iostream>\n",
        "int main() {\n",
        "    std::cout << \"Hello world\\n\";\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-162d861j\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-162d861j\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp36-none-any.whl size=4307 sha256=801bf6cbe18412ebc8691bcd3172a7fb9c1d6c1b30193705fd2c8c7e1e2f2427\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cdlovexn/wheels/10/c2/05/ca241da37bff77d60d31a9174f988109c61ba989e4d4650516\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAgUrxO0swry"
      },
      "source": [
        "## Test CUDA Functionality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaS5SsgZs4VR",
        "outputId": "3bebbd2d-744c-48d1-dd53-202922f89f45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%cu\n",
        "#include <iostream>\n",
        "int main() {\n",
        "\n",
        "  std::cout << \"Hello World its Siraj\";\n",
        "  return 0;\n",
        "\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello World its Siraj\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gkcUIQhtCwh"
      },
      "source": [
        "## Let's write an 'add 2 elements' script in C!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ghq6v-6UtGzb",
        "outputId": "e2a0e227-5bf9-4c91-d1b6-06a690a92b0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%cu\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "// function to add the elements of two arrays\n",
        "__global__ void add(int n, float *x, float *y)\n",
        "{\n",
        "  for (int i = 0; i < n; i++)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "\n",
        "  int N = 1<<20; // 1M elements\n",
        "\n",
        " float *x, *y;\n",
        "\n",
        "\n",
        " cudaMallocManaged(&x, N*sizeof(float))\n",
        " cudaMallocManaged(&y, N*sizeof(float))\n",
        "\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "\n",
        "// Run kernel on 1M elements on the CPU\n",
        "  add(N, x, y);\n",
        "\n",
        "\n",
        "\n",
        " cudaDeviceSynchronize()\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "\n",
        "  // Free memory\n",
        "  delete [] x;\n",
        "  delete [] y;\n",
        "\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max error: 0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwVSOvobxvoE"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true 1.cu -o 1 -lcudadevrt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49nMO4Iax1U_",
        "outputId": "121ab403-53a1-421d-dccc-b892aad73a2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!nvprof ./1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max error: 0\n",
            "======== Warning: No profile data collected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTgCrVemNVjn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "dafdc29e-8102-4bfd-af0f-049015237f9f"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true 2.cu -o 2 -lcudadevrt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[01m\u001b[Kgcc:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K2.cu: No such file or directory\n",
            "\u001b[01m\u001b[Kgcc:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[K-x c++\u001b[m\u001b[K’ after last input file has no effect\n",
            "\u001b[01m\u001b[Kgcc:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kno input files\n",
            "compilation terminated.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-nZ9IuFNYDz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "4ddaebb6-0a05-41c2-a78e-aef64c100fe0"
      },
      "source": [
        "!nvprof ./2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==240== NVPROF is profiling process 240, command: ./2\n",
            "Max error: 0\n",
            "==240== Profiling application: ./2\n",
            "==240== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  126.13ms         1  126.13ms  126.13ms  126.13ms  add(int, float*, float*)\n",
            "      API calls:   72.66%  338.61ms         2  169.31ms  45.512us  338.57ms  cudaMallocManaged\n",
            "                   27.07%  126.15ms         1  126.15ms  126.15ms  126.15ms  cudaDeviceSynchronize\n",
            "                    0.14%  646.00us         2  323.00us  299.14us  346.86us  cudaFree\n",
            "                    0.08%  362.23us         1  362.23us  362.23us  362.23us  cuDeviceTotalMem\n",
            "                    0.03%  157.58us        96  1.6410us     141ns  69.728us  cuDeviceGetAttribute\n",
            "                    0.01%  41.781us         1  41.781us  41.781us  41.781us  cudaLaunchKernel\n",
            "                    0.01%  24.340us         1  24.340us  24.340us  24.340us  cuDeviceGetName\n",
            "                    0.00%  2.8220us         1  2.8220us  2.8220us  2.8220us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.1630us         3     721ns     174ns  1.3730us  cuDeviceGetCount\n",
            "                    0.00%  1.8530us         2     926ns     256ns  1.5970us  cuDeviceGet\n",
            "                    0.00%     397ns         1     397ns     397ns     397ns  cuDeviceGetUuid\n",
            "\n",
            "==240== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "      48  170.67KB  4.0000KB  0.9961MB  8.000000MB  809.3440us  Host To Device\n",
            "      24  170.67KB  4.0000KB  0.9961MB  4.000000MB  360.3200us  Device To Host\n",
            "      12         -         -         -           -  2.450592ms  Gpu page fault groups\n",
            "Total CPU Page faults: 36\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dmPhOCYNcLj"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true 3.cu -o 3 -lcudadevrt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpEzj4zONe7l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "368328ab-2a96-4654-94a4-d9197fc9da9c"
      },
      "source": [
        "!nvprof ./3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==294== NVPROF is profiling process 294, command: ./3\n",
            "Max error: 0\n",
            "==294== Profiling application: ./3\n",
            "==294== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  3.9588ms         1  3.9588ms  3.9588ms  3.9588ms  add(int, float*, float*)\n",
            "      API calls:   98.01%  256.85ms         2  128.42ms  28.643us  256.82ms  cudaMallocManaged\n",
            "                    1.51%  3.9678ms         1  3.9678ms  3.9678ms  3.9678ms  cudaDeviceSynchronize\n",
            "                    0.26%  670.17us         2  335.08us  312.04us  358.13us  cudaFree\n",
            "                    0.14%  368.57us         1  368.57us  368.57us  368.57us  cuDeviceTotalMem\n",
            "                    0.05%  134.93us        96  1.4050us     126ns  56.333us  cuDeviceGetAttribute\n",
            "                    0.01%  39.165us         1  39.165us  39.165us  39.165us  cudaLaunchKernel\n",
            "                    0.01%  23.109us         1  23.109us  23.109us  23.109us  cuDeviceGetName\n",
            "                    0.00%  3.1410us         1  3.1410us  3.1410us  3.1410us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.7400us         3     580ns     159ns  1.2610us  cuDeviceGetCount\n",
            "                    0.00%     930ns         2     465ns     182ns     748ns  cuDeviceGet\n",
            "                    0.00%     256ns         1     256ns     256ns     256ns  cuDeviceGetUuid\n",
            "\n",
            "==294== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "      48  170.67KB  4.0000KB  0.9961MB  8.000000MB  811.7120us  Host To Device\n",
            "      24  170.67KB  4.0000KB  0.9961MB  4.000000MB  360.8320us  Device To Host\n",
            "      12         -         -         -           -  2.243648ms  Gpu page fault groups\n",
            "Total CPU Page faults: 36\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ7wzBQqNh35"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true 4.cu -o 4 -lcudadevrt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4WScDesNlQl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "21764e87-7781-483f-a905-0ebae5fc2f32"
      },
      "source": [
        "!nvprof ./4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==351== NVPROF is profiling process 351, command: ./4\n",
            "Max error: 0\n",
            "==351== Profiling application: ./4\n",
            "==351== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  3.2025ms         1  3.2025ms  3.2025ms  3.2025ms  add(int, float*, float*)\n",
            "      API calls:   98.41%  274.11ms         2  137.06ms  35.345us  274.08ms  cudaMallocManaged\n",
            "                    1.15%  3.2131ms         1  3.2131ms  3.2131ms  3.2131ms  cudaDeviceSynchronize\n",
            "                    0.21%  575.39us         2  287.70us  258.15us  317.25us  cudaFree\n",
            "                    0.14%  400.01us         1  400.01us  400.01us  400.01us  cuDeviceTotalMem\n",
            "                    0.06%  155.55us        96  1.6200us     152ns  64.004us  cuDeviceGetAttribute\n",
            "                    0.02%  42.642us         1  42.642us  42.642us  42.642us  cudaLaunchKernel\n",
            "                    0.01%  23.546us         1  23.546us  23.546us  23.546us  cuDeviceGetName\n",
            "                    0.00%  2.9540us         1  2.9540us  2.9540us  2.9540us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.8260us         3     608ns     149ns  1.3460us  cuDeviceGetCount\n",
            "                    0.00%  1.0550us         2     527ns     215ns     840ns  cuDeviceGet\n",
            "                    0.00%     258ns         1     258ns     258ns     258ns  cuDeviceGetUuid\n",
            "\n",
            "==351== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "     106  77.282KB  4.0000KB  960.00KB  8.000000MB  959.5520us  Host To Device\n",
            "      24  170.67KB  4.0000KB  0.9961MB  4.000000MB  360.1280us  Device To Host\n",
            "      11         -         -         -           -  3.156928ms  Gpu page fault groups\n",
            "Total CPU Page faults: 36\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJUKJyYON6dC"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}