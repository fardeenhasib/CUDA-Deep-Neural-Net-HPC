{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpqtrrtDm9Y6"
      },
      "source": [
        "# CUDA Neural Networks\n",
        "\n",
        "## Demo - Binary number classification with a CUDA Neural Network\n",
        "\n",
        "![alt text](https://i.imgur.com/FhGiI9R.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68EbUYm_nfOS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "eb9b0f0b-8817-4f93-91a5-499b9562856d"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true simple.cu -o simple -lcudadevrt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[01m\u001b[Kgcc:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[Ksimple.cu: No such file or directory\n",
            "\u001b[01m\u001b[Kgcc:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[K-x c++\u001b[m\u001b[K’ after last input file has no effect\n",
            "\u001b[01m\u001b[Kgcc:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kno input files\n",
            "compilation terminated.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c8SOMNSns81",
        "outputId": "506a745c-a440-43fe-91ed-48b4b683c4ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "!./simple"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction[0] : 0.060997 True Value[0] : 0.000000 Error[0] : 0.060997\n",
            "Prediction[1] : 0.076193 True Value[1] : 0.000000 Error[1] : 0.076193\n",
            "Prediction[2] : 0.927551 True Value[2] : 1.000000 Error[2] : -0.072449\n",
            "Prediction[3] : 0.918263 True Value[3] : 1.000000 Error[3] : -0.081737\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUIZlWy3s9G8",
        "outputId": "429fdacd-2dd2-48c4-a08e-22bccd648c2d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIYndb9LtKsi",
        "outputId": "9206cf79-aa1b-4af4-e6d8-6169f3ddfd2d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-j2jvh55e\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-j2jvh55e\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 0a71d56e5dce3ff1f0dd2c47c29367629262f527\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4295 sha256=61bf87063ed9d91c44453bc8df22c9ade90c3b81fa4dc98382426712a859809f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-md4tz_u1/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVA5N_DTtOLe",
        "outputId": "88ff9e4e-6cfb-4820-c3c1-5c903f6151d1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H5cJHqboO_S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4ff3f84-129d-4806-956b-d689f1a4ad07"
      },
      "source": [
        "%%cu\n",
        "#include <omp.h>\n",
        "#include <bits/stdc++.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "#include <time.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "#define MAX_MATRIX_SIZE 65536\n",
        "#define DEBUG false\n",
        "#define BLOCK_SIZE 16\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void kMartixByMatrixElementwise(const int nThreads, const float *m1, const float *m2, float *output) {\n",
        "    /*  Computes the product of two arrays (elementwise multiplication).\n",
        "     Inputs:\n",
        "     m1: array\n",
        "     m2: array\n",
        "     output: array,the results of the multiplication are to be stored here\n",
        "    */\n",
        "\tfor (int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\t i < nThreads;\n",
        "\t\t i += blockDim.x * gridDim.x)\n",
        "\t  {\n",
        "\t\toutput[i] = m1[i] * m2[i];\n",
        "\t  }\n",
        "}\n",
        "\n",
        "__device__ float* dMartixByMatrixElementwise(const float *m1, const float *m2, float *output, const int width, const int height){\n",
        "\n",
        "\tkMartixByMatrixElementwise <<< width, height >>> ( width * height, m1, m2, output );\n",
        "    cudaDeviceSynchronize();\n",
        "    return output;\n",
        "}\n",
        "\n",
        "__global__ void kMartixSubstractMatrix(const int nThreads, const float *m1, const float *m2, float *output) {\n",
        "    /*  Computes the (elementwise) difference between two arrays\n",
        "     Inputs:\n",
        "     m1: array\n",
        "     m2: array\n",
        "     output: array,the results of the computation are to be stored here\n",
        "     */\n",
        "\n",
        "\tfor (int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\t i < nThreads;\n",
        "\t\t i += blockDim.x * gridDim.x)\n",
        "\t  {\n",
        "\t\toutput[i] = m1[i] - m2[i];\n",
        "\t  }\n",
        "}\n",
        "\n",
        "__device__ float* dMartixSubstractMatrix(const float *m1, const float *m2, float *output, const int width, const int height){\n",
        "\n",
        "\tkMartixSubstractMatrix <<< width, height >>> ( width * height, m1, m2, output );\n",
        "    cudaDeviceSynchronize();\n",
        "    return output;\n",
        "}\n",
        "\n",
        "__global__ void kSigmoid(const int nThreads, float const *input, float *output){\n",
        "    /*  Computes the value of the sigmoid function f(x) = 1/(1 + e^-x).\n",
        "     Inputs:\n",
        "     input: array\n",
        "     output: array, the results of the computation are to be stored here\n",
        "    */\n",
        "\n",
        "\tfor (int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\t i < nThreads;\n",
        "\t\t i += blockDim.x * gridDim.x)\n",
        "\t  {\n",
        "\t\toutput[i] = 1.0 / (1.0 + std::exp(-input[i]));\n",
        "\t  }\n",
        "}\n",
        "\n",
        "__device__ void dSigmoid(float const *input, float *output, const int height, const int width){\n",
        "\n",
        "\tkSigmoid <<< height, width >>> (height * width, input, output);\n",
        "\tcudaDeviceSynchronize();\n",
        "}\n",
        "\n",
        "__global__ void kSigmoid_d(const int nThreads, float const *input, float *output) {\n",
        "\t/*  Computes the value of the sigmoid function derivative f'(x) = f(x)(1 - f(x)),\n",
        "\t    where f(x) is sigmoid function.\n",
        "\t    Inputs:\n",
        "\t    input: array\n",
        "\t    output: array, the results of the computation are to be stored here:\n",
        "\t    \t\tx(1 - x) for every element of the input matrix m1.\n",
        "\t*/\n",
        "\n",
        "\tfor (int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\t i < nThreads;\n",
        "\t\t i += blockDim.x * gridDim.x)\n",
        "\t  {\n",
        "\t\toutput[i] = input[i] * (1 - input[i]);\n",
        "\t  }\n",
        "}\n",
        "\n",
        "__device__ float* dSigmoid_d(float const *input, float *output, const int rows, const int columns){\n",
        "\tkSigmoid_d <<< rows, columns >>> (rows*columns, input, output);\n",
        "\tcudaDeviceSynchronize();\n",
        "\treturn output;\n",
        "}\n",
        "\n",
        "__global__ void kDot(const int nThreads, const float *m1, const float *m2, float *output, const int m1_rows , const int m1_columns, const int m2_columns ){\n",
        "\t/*  Computes the product of two matrices: m1 x m2.\n",
        "\t   \tInputs:\n",
        "\t    m1: array, left matrix of size m1_rows x m1_columns\n",
        "\t    m2: array, right matrix of size m1_columns x m2_columns (the number of rows in the right matrix\n",
        "\t    must be equal to the number of the columns in the left one)\n",
        "\t    output: array, the results of the computation are to be stored here:\n",
        "\t    \t\tm1 * m2, product of two arrays m1 and m2, a matrix of size m1_rows x m2_columns\n",
        "\t    m1_rows: int, number of rows in the left matrix m1\n",
        "\t    m1_columns: int, number of columns in the left matrix m1\n",
        "\t    m2_columns: int, number of columns in the right matrix m2\n",
        "\t*/\n",
        "\n",
        "\tfor (int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\t i < nThreads;\n",
        "\t\t i += blockDim.x * gridDim.x)\n",
        "\t{\n",
        "\t    int r = (int)i / m2_columns;\n",
        "\t    int c = i % m2_columns;\n",
        "\t    float t_output = 0.f;\n",
        "\n",
        "\t    for( int k = 0; k < m1_columns; ++k ) {\n",
        "\t        t_output += m1[ r * m1_columns + k ] * m2[ k * m2_columns + c ];\n",
        "\t    }\n",
        "\n",
        "\t    output[i] = t_output;\n",
        "\t}\n",
        "}\n",
        "\n",
        "__device__ float* dDot(const float *m1, const float *m2, float *output, const int m1_rows , const int m1_columns, const int m2_columns ){\n",
        "\n",
        "\tkDot <<< m1_rows, m2_columns >>> (m1_rows * m2_columns, m1, m2, output, m1_rows , m1_columns, m2_columns );\n",
        "\tcudaDeviceSynchronize();\n",
        "\treturn output;\n",
        "}\n",
        "\n",
        "__global__ void kDot_m1_m2T(const int nThreads, const float *m1, const float *m2, float *output, const int m1_columns, const int m2_rows ){\n",
        "\t/*  Updates the output matrix with the product of two matrices: m1 and m2 transposed.\n",
        "\t   \tInputs:\n",
        "\t    m1: array, left matrix of size m1_rows x m1_columns\n",
        "\t    m2: array, right matrix of size m2_rows x m1_columns (m2 transposed will be of size m1_columns x m2_rows)\n",
        "\t    output: array, the results of the computation are to be stored here:\n",
        "\t    \t\tm1 * m2, product of two arrays m1 and m2, a matrix of size m1_rows x m2_rows\n",
        "\t    m1_columns: int, number of columns in the left matrix m1\n",
        "\t    m2_rows: int, number of rows in the left matrix m2\n",
        "\t*/\n",
        "\n",
        "\tfor (int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\t i < nThreads;\n",
        "\t\t i += blockDim.x * gridDim.x)\n",
        "\t{\n",
        "\t\tint r = (int)i / m2_rows;\n",
        "\t\tint c = i % m2_rows;\n",
        "\t\tfloat t_output = 0.0;\n",
        "\t\tint id_T;\n",
        "\n",
        "\t\tfor( int k = 0; k < m1_columns; ++k ) {\n",
        "\t\t\tid_T = c * m1_columns + k;\n",
        "\t\t\tt_output += m1[ r * m1_columns + k ] * m2[ id_T ];\n",
        "\t\t}\n",
        "\n",
        "\t\toutput[i] = t_output;\n",
        "\t}\n",
        "}\n",
        "\n",
        "__device__ float* dDot_m1_m2T(const float *m1, const float *m2, float *output, const int m1_rows , const int m1_columns, const int m2_rows )\n",
        "{\n",
        "\tkDot_m1_m2T <<< m1_rows, m2_rows >>> ( m1_rows * m2_rows, m1, m2, output, m1_columns, m2_rows );\n",
        "\tcudaDeviceSynchronize();\n",
        "\treturn output;\n",
        "}\n",
        "\n",
        "__global__ void kDot_m1T_m2(const int nThreads, const float *m1, const float *m2, float *output, const int m1_rows,\n",
        "\t\t\t\t\t\t\tconst int m1_columns, const int m2_columns ){\n",
        "\t/*  Increments the output matrix with the product of two matrices: m1 transposed and m2.\n",
        "\t   \tInputs:\n",
        "\t    m1: array, left matrix of size m1_rows x m1_columns (m1 transposed will be of size m1_columns x m1_rows)\n",
        "\t    m2: array, right matrix of size m1_rows x m2_columns\n",
        "\t    output: array, the results of the computation are to be stored here:\n",
        "\t    \t\tm1 * m2, product of two arrays m1 and m2, a matrix of size m1_columns x m2_columns\n",
        "\t    m1_rows: int, number of rows in the left matrix m1\n",
        "\t    m1_columns: int, number of columns in the left matrix m1\n",
        "\t    m2_rows: int, number of rows in the left matrix m2\n",
        "\t*/\n",
        "\n",
        "\tfor (int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\t i < nThreads;\n",
        "\t\t i += blockDim.x * gridDim.x)\n",
        "\t{\n",
        "\t    int r = (int)i / m2_columns;\n",
        "\t    int c = i % m2_columns;\n",
        "\t    int id_T;\n",
        "\t    float t_output = 0.0;\n",
        "\n",
        "\t    for( int k = 0; k < m1_rows; ++k ) {\n",
        "\t    \tid_T = k * m1_columns + r;\n",
        "\t        t_output += m1[ id_T ] * m2[ k * m2_columns + c ];\n",
        "\t    }\n",
        "\n",
        "\t    output[i] += t_output;\n",
        "\t}\n",
        "}\n",
        "\n",
        "__device__ void dDot_m1T_m2(const float *m1, const float *m2, float *output, const int m1_height , const int m1_width, const int m2_width )\n",
        "{\n",
        "\tkDot_m1T_m2 <<< m1_width, m2_width >>> (m1_width * m2_width, m1, m2, output, m1_height, m1_width, m2_width );\n",
        "\tcudaDeviceSynchronize();\n",
        "}\n",
        "\n",
        "__device__ void kPrintMatrix (const float* M, int h, int w) {\n",
        "    /*  Prints out the input array as h x w matrix.\n",
        "     Inputs:\n",
        "     m: vector, matrix of size n_rows x n_columns\n",
        "     h: int, number of rows in the matrix M\n",
        "     w: int, number of columns in the matrix M\n",
        "     */\n",
        "\tfor (int i = 0; i < h; i++){\n",
        "\t\tfor (int j = 0; j < w; j++){\n",
        "\t\t\tprintf(\"%f  \", M[i*w+j]);\n",
        "\t\t}\n",
        "\t\tprintf(\"\\n\");\n",
        "\t}\n",
        "\tprintf(\"\\n\");\n",
        "}\n",
        "\n",
        "__global__ void kFit(\tconst float* X, const int X_w, const int X_h,\n",
        "\t\t\t\t\t\tconst float* y, const int y_w,\n",
        "\t\t\t\t\t\tfloat* l1, const int l1_w, float* l_1_d,\n",
        "\t\t\t\t\t\tfloat* pred, float* pred_d,\n",
        "\t\t\t\t\t\tfloat* W0,\n",
        "\t\t\t\t\t\tfloat* W1,\n",
        "\t\t\t\t\t\tfloat* buffer\n",
        "\t\t\t\t\t\t)\n",
        "{\n",
        "\tfor (unsigned i = 0; i < 50; ++i) {\n",
        "\n",
        "        dSigmoid(dDot(X, W0, l1, X_h, X_w, l1_w), l1, X_h, l1_w);\n",
        "        dSigmoid(dDot(l1, W1, pred, X_h, l1_w, y_w), pred, X_h, y_w);\n",
        "        dMartixByMatrixElementwise(dMartixSubstractMatrix(y, pred, pred_d, X_h, y_w), dSigmoid_d(pred, buffer, X_h, y_w), pred_d, X_h, y_w );\n",
        "        dMartixByMatrixElementwise(dDot_m1_m2T(pred_d, W1, l_1_d, X_h, y_w, l1_w), dSigmoid_d(l1, buffer, X_h, l1_w), l_1_d, X_h, l1_w);\n",
        "        dDot_m1T_m2( l1, pred_d, W1, X_h, l1_w, y_w );\n",
        "        dDot_m1T_m2( X, l_1_d, W0, X_h, X_w, l1_w );\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "int matrix_size, terminal_matrix_size;\n",
        "\n",
        "\n",
        "\n",
        "void print(int n, int** mat)\n",
        "{\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        for (int j = 0; j < n; j++) {\n",
        "            cout << mat[i][j] << \" \";\n",
        "        }\n",
        "        cout << endl;\n",
        "    }\n",
        "    cout << endl;\n",
        "}\n",
        "\n",
        "\n",
        "int** getSlice(int n, int** mat, int offseti, int offsetj)\n",
        "{\n",
        "    int m = n / 2;\n",
        "\n",
        "    int** slice = (int**)malloc(n * sizeof(int*));\n",
        "    int* data_slice = (int*)malloc(n * n * sizeof(int));\n",
        "\n",
        "    for (int i = 0; i < n; i++)\n",
        "    {\n",
        "        slice[i] = &(data_slice[n * i]);\n",
        "    }\n",
        "\n",
        "    for (int i = 0; i < m; i++)\n",
        "    {\n",
        "        for (int j = 0; j < m; j++)\n",
        "        {\n",
        "            slice[i][j] = mat[offseti + i][offsetj + j];\n",
        "        }\n",
        "    }\n",
        "    return slice;\n",
        "}\n",
        "\n",
        "int** addMatrices(int n, int** mat1, int** mat2, bool add)\n",
        "{\n",
        "    int** result = (int**)malloc(n * sizeof(int*));\n",
        "    int* data_result = (int*)malloc(n * n * sizeof(int));\n",
        "\n",
        "    for (int i = 0; i < n; i++)\n",
        "    {\n",
        "        result[i] = &(data_result[n * i]);\n",
        "    }\n",
        "\n",
        "    for (int i = 0; i < n; i++)\n",
        "    {\n",
        "        for (int j = 0; j < n; j++)\n",
        "        {\n",
        "            if (add)\n",
        "                result[i][j] = mat1[i][j] + mat2[i][j];\n",
        "            else\n",
        "                result[i][j] = mat1[i][j] - mat2[i][j];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return result;\n",
        "}\n",
        "\n",
        "int** combineMatrices(int m, int** c11, int** c12, int** c21, int** c22)\n",
        "{\n",
        "    int n = 2 * m;\n",
        "\n",
        "    int** result = (int**)malloc(n * sizeof(int*));\n",
        "    int* data_result = (int*)malloc(n * n * sizeof(int));\n",
        "\n",
        "    for (int i = 0; i < n; i++)\n",
        "    {\n",
        "        result[i] = &(data_result[n * i]);\n",
        "    }\n",
        "\n",
        "    for (int i = 0; i < n; i++)\n",
        "    {\n",
        "        for (int j = 0; j < n; j++)\n",
        "        {\n",
        "            if (i < m && j < m)\n",
        "                result[i][j] = c11[i][j];\n",
        "            else if (i < m)\n",
        "                result[i][j] = c12[i][j - m];\n",
        "            else if (j < m)\n",
        "                result[i][j] = c21[i - m][j];\n",
        "            else\n",
        "                result[i][j] = c22[i - m][j - m];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return result;\n",
        "}\n",
        "\n",
        "\n",
        "int** naive(int n, int** mat1, int** mat2)\n",
        "{\n",
        "    int** prod = (int**)malloc(n * sizeof(int*));\n",
        "    int* data_prod = (int*)malloc(n * n * sizeof(int));\n",
        "\n",
        "    for (int i = 0; i < n; i++)\n",
        "    {\n",
        "        prod[i] = &(data_prod[n * i]);\n",
        "    }\n",
        "\n",
        "    for (int i = 0; i < n; i++)\n",
        "    {\n",
        "        for (int j = 0; j < n; j++)\n",
        "        {\n",
        "            prod[i][j] = 0;\n",
        "            for (int k = 0; k < n; k++)\n",
        "            {\n",
        "                prod[i][j] += mat1[i][k] * mat2[k][j];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return prod;\n",
        "}\n",
        "\n",
        "__global__ void multiply(int *left, int *right, int *res, int dim) {\n",
        "\n",
        "    int i,j;\n",
        "    int temp = 0;\n",
        "\n",
        "    __shared__ float Left_shared_t [BLOCK_SIZE][BLOCK_SIZE];\n",
        "    __shared__ float Right_shared_t[BLOCK_SIZE][BLOCK_SIZE];\n",
        "\n",
        "    // Row i of matrix left\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "\n",
        "    for (int tileNUM = 0; tileNUM < gridDim.x; tileNUM++) {\n",
        "\n",
        "        // Column j of matrix left\n",
        "        j = tileNUM * BLOCK_SIZE + threadIdx.x;\n",
        "        i = tileNUM * BLOCK_SIZE + threadIdx.y;\n",
        "        // Load left[i][j] to shared mem\n",
        "\n",
        "        Left_shared_t[threadIdx.y][threadIdx.x] = left[row * dim + j];// Coalesced access\n",
        "        // Load right[i][j] to shared mem\n",
        "\n",
        "        Right_shared_t[threadIdx.y][threadIdx.x] = right[i * dim + col]; // Coalesced access\n",
        "        // Synchronize before computation\n",
        "        __syncthreads();\n",
        "\n",
        "        // Accumulate one tile of res from tiles of left and right in shared mem\n",
        "        for (int k = 0; k < BLOCK_SIZE; k++) {\n",
        "\n",
        "            temp += Left_shared_t[threadIdx.y][k] * Right_shared_t[k][threadIdx.x]; //no shared memory bank conflict\n",
        "        }\n",
        "        // Synchronize\n",
        "        __syncthreads();\n",
        "    }\n",
        "    // Store accumulated value to res\n",
        "    res[row * dim + col] = temp;\n",
        "}\n",
        "\n",
        "int** cudaNaive(int n, int** mat1, int** mat2)\n",
        "{\n",
        "    size_t bytes;\n",
        "    bytes = n*n * sizeof(int);\n",
        "\n",
        "    int* h_mat1 = (int*)malloc(bytes);\n",
        "\n",
        "    for(int i=0;i<n;i++){\n",
        "        for(int j=0;j<n;j++){\n",
        "            h_mat1[i*n + j] = mat1[i][j];\n",
        "            //cout<< h_mat1[i*n +j]<<\" \";\n",
        "        }\n",
        "        //cout<<endl;\n",
        "    }\n",
        "\n",
        "    int* h_mat2 = (int*)malloc(bytes);\n",
        "    for(int i=0;i<n;i++){\n",
        "        for(int j=0;j<n;j++){\n",
        "            h_mat2[i*n + j] = mat2[i][j];\n",
        "           //cout<< h_mat2[i*n +j]<<\" \";\n",
        "        }\n",
        "        //cout<<endl;\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "    int* h_product = (int*)malloc(bytes);\n",
        "    int *d_mat1, *d_mat2, *d_product;\n",
        "\n",
        "    cudaMalloc((void**)&d_mat1, bytes);\n",
        "    cudaMalloc((void**)&d_mat2, bytes);\n",
        "    cudaMalloc((void**)&d_product, bytes);\n",
        "\n",
        "    cudaMemcpy(d_mat1, h_mat1, bytes, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_mat2, h_mat2, bytes, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_product, h_product, bytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "    int block_size = min(n, 16);\n",
        "    //dim3 Block_dim(BLOCK_SIZE, BLOCK_SIZE);\n",
        "    dim3 Block_dim(block_size, block_size);\n",
        "    //Grid dimension is found by dividing matrix dimension to block_size\n",
        "    dim3 Grid_dim(n / block_size, n / block_size);\n",
        "\n",
        "    multiply<<<Grid_dim, Block_dim>>>(d_mat1, d_mat2, d_product, n);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    cudaMemcpy(h_product, d_product, bytes, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    int** product = (int**)malloc(n * sizeof(int*));\n",
        "    int* data_product = (int*)malloc(n * n * sizeof(int));\n",
        "    for (int i = 0; i < n; i++)\n",
        "    {\n",
        "        product[i] = &(data_product[n * i]);\n",
        "    }\n",
        "\n",
        "    for(int i=0;i<n;i++){\n",
        "        for(int j=0;j<n;j++){\n",
        "            // cout<< h_product[i*n+j]<<\" \";\n",
        "            product[i][j] = h_product[i*n + j];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    cudaFree(d_mat1);\n",
        "    cudaFree(d_mat2);\n",
        "    cudaFree(d_product);\n",
        "\n",
        "    free(h_mat1);\n",
        "    free(h_mat2);\n",
        "\n",
        "    return product;\n",
        "}\n",
        "\n",
        "\n",
        "int** strassen(int n, int** mat1, int** mat2)\n",
        "{\n",
        "\n",
        "    if (n <= terminal_matrix_size)\n",
        "    {\n",
        "        return cudaNaive(n, mat1, mat2);\n",
        "    }\n",
        "\n",
        "    int m = n / 2;\n",
        "\n",
        "    int** a = getSlice(n, mat1, 0, 0);\n",
        "    int** b = getSlice(n, mat1, 0, m);\n",
        "    int** c = getSlice(n, mat1, m, 0);\n",
        "    int** d = getSlice(n, mat1, m, m);\n",
        "    int** e = getSlice(n, mat2, 0, 0);\n",
        "    int** f = getSlice(n, mat2, 0, m);\n",
        "    int** g = getSlice(n, mat2, m, 0);\n",
        "    int** h = getSlice(n, mat2, m, m);\n",
        "\n",
        "    int** bds = addMatrices(m, b, d, false);\n",
        "    int** gha = addMatrices(m, g, h, true);\n",
        "    int** s1 = strassen(m, bds, gha);\n",
        "\n",
        "    free(bds[0]); free(bds);\n",
        "\n",
        "    free(gha[0]); free(gha);\n",
        "\n",
        "    int** ada = addMatrices(m, a, d, true);\n",
        "    int** eha = addMatrices(m, e, h, true);\n",
        "    int** s2 = strassen(m, ada, eha);\n",
        "\n",
        "    free(ada[0]); free(ada);\n",
        "\n",
        "    free(eha[0]); free(eha);\n",
        "\n",
        "    int** acs = addMatrices(m, a, c, false);\n",
        "    int** efa = addMatrices(m, e, f, true);\n",
        "    int** s3 = strassen(m, acs, efa);\n",
        "\n",
        "    free(acs[0]); free(acs);\n",
        "\n",
        "    free(efa[0]); free(efa);\n",
        "\n",
        "    int** aba = addMatrices(m, a, b, true);\n",
        "    int** s4 = strassen(m, aba, h);\n",
        "\n",
        "    free(aba[0]); free(aba);\n",
        "    free(b[0]); free(b);\n",
        "\n",
        "    int** fhs = addMatrices(m, f, h, false);\n",
        "    int** s5 = strassen(m, a, fhs);\n",
        "\n",
        "    free(fhs[0]); free(fhs);\n",
        "    free(a[0]); free(a);\n",
        "    free(f[0]); free(f);\n",
        "    free(h[0]); free(h);\n",
        "\n",
        "    int** ges = addMatrices(m, g, e, false);\n",
        "    int** s6 = strassen(m, d, ges);\n",
        "\n",
        "    free(ges[0]); free(ges);\n",
        "    free(g[0]); free(g);\n",
        "\n",
        "    int** cda = addMatrices(m, c, d, true);\n",
        "    int** s7 = strassen(m, cda, e);\n",
        "\n",
        "    free(cda[0]); free(cda);\n",
        "    free(c[0]); free(c);\n",
        "    free(d[0]); free(d);\n",
        "    free(e[0]); free(e);\n",
        "\n",
        "    int** s1s2a = addMatrices(m, s1, s2, true);\n",
        "    int** s6s4s = addMatrices(m, s6, s4, false);\n",
        "    int** c11 = addMatrices(m, s1s2a, s6s4s, true);\n",
        "\n",
        "    free(s1s2a[0]); free(s1s2a);\n",
        "    free(s6s4s[0]); free(s6s4s);\n",
        "    free(s1[0]); free(s1);\n",
        "\n",
        "    int** c12 = addMatrices(m, s4, s5, true);\n",
        "    free(s4[0]); free(s4);\n",
        "\n",
        "    int** c21 = addMatrices(m, s6, s7, true);\n",
        "    free(s6[0]); free(s6);\n",
        "\n",
        "    int** s2s3s = addMatrices(m, s2, s3, false);\n",
        "    int** s5s7s = addMatrices(m, s5, s7, false);\n",
        "    int** c22 = addMatrices(m, s2s3s, s5s7s, true);\n",
        "\n",
        "    free(s2s3s[0]); free(s2s3s);\n",
        "    free(s5s7s[0]); free(s5s7s);\n",
        "    free(s2[0]); free(s2);\n",
        "    free(s3[0]); free(s3);\n",
        "    free(s5[0]); free(s5);\n",
        "    free(s7[0]); free(s7);\n",
        "\n",
        "    int** prod = combineMatrices(m, c11, c12, c21, c22);\n",
        "\n",
        "    free(c11[0]); free(c11);\n",
        "    free(c12[0]); free(c12);\n",
        "    free(c21[0]); free(c21);\n",
        "    free(c22[0]); free(c22);\n",
        "\n",
        "    return prod;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "__global__ void copy_row(int * mat, int * transpose, int nx, int ny)\n",
        "{\n",
        "\tint ix = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\tint iy = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "\tif (ix < nx && iy < ny)\n",
        "\t{\n",
        "\t\ttranspose[iy * nx + ix] = mat[iy * nx + ix];\n",
        "\t}\n",
        "}\n",
        "\n",
        "__global__ void copy_column(int * mat, int * transpose, int nx, int ny)\n",
        "{\n",
        "\tint ix = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\tint iy = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "\tif (ix < nx && iy < ny)\n",
        "\t{\n",
        "\t\ttranspose[ix * ny + iy] = mat[ix * ny + iy];\n",
        "\t}\n",
        "}\n",
        "\n",
        "__global__ void transpose_read_row_write_column(int * mat, int * transpose, int nx, int ny)\n",
        "{\n",
        "\tint ix = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\tint iy = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "\tif (ix < nx && iy < ny)\n",
        "\t{\n",
        "\t\ttranspose[ix * ny + iy] = mat[iy * nx + ix];\n",
        "\t}\n",
        "}\n",
        "\n",
        "__global__ void transpose_read_column_write_row(int * mat, int * transpose, int nx, int ny)\n",
        "{\n",
        "\tint ix = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\tint iy = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "\tif (ix < nx && iy < ny)\n",
        "\t{\n",
        "\t\ttranspose[iy * nx + ix] = mat[ix * ny + iy];\n",
        "\t}\n",
        "}\n",
        "\n",
        "__global__ void transpose_unroll4_row(int * mat, int * transpose, int nx, int ny)\n",
        "{\n",
        "\tint ix = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n",
        "\tint iy = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "\tint ti = iy * nx + ix;\n",
        "\tint to = ix * ny + iy;\n",
        "\n",
        "\tif (ix + 3 * blockDim.x < nx && iy < ny)\n",
        "\t{\n",
        "\t\ttranspose[to]\t\t\t\t\t\t= mat[ti];\n",
        "\t\ttranspose[to + ny*blockDim.x]\t\t= mat[ti + blockDim.x];\n",
        "\t\ttranspose[to + ny * 2 * blockDim.x] = mat[ti + 2 * blockDim.x];\n",
        "\t\ttranspose[to + ny * 3 * blockDim.x] = mat[ti + 3 * blockDim.x];\n",
        "\t}\n",
        "}\n",
        "\n",
        "__global__ void transpose_unroll4_col(int * mat, int * transpose, int nx, int ny)\n",
        "{\n",
        "\tint ix = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n",
        "\tint iy = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "\tint ti = iy * nx + ix;\n",
        "\tint to = ix * ny + iy;\n",
        "\n",
        "\tif (ix + 3 * blockDim.x < nx && iy < ny)\n",
        "\t{\n",
        "\t\ttranspose[ti] = mat[to];\n",
        "\t\ttranspose[ti + blockDim.x] = mat[to + blockDim.x*ny];\n",
        "\t\ttranspose[ti + 2 * blockDim.x] = mat[to + 2 * blockDim.x*ny];\n",
        "\t\ttranspose[ti + 3 * blockDim.x] = mat[to + 3 * blockDim.x*ny];\n",
        "\t}\n",
        "}\n",
        "\n",
        "__global__ void transpose_diagonal_row(int * mat, int * transpose, int nx, int ny)\n",
        "{\n",
        "\tint blk_x = blockIdx.x;\n",
        "\tint blk_y = (blockIdx.x + blockIdx.y) % gridDim.x;\n",
        "\n",
        "\tint ix = blockIdx.x * blk_x + threadIdx.x;\n",
        "\tint iy = blockIdx.y * blk_y + threadIdx.y;\n",
        "\n",
        "\tif (ix < nx && iy < ny)\n",
        "\t{\n",
        "\t\ttranspose[ix * ny + iy] = mat[iy * nx + ix];\n",
        "\t}\n",
        "}\n",
        "\n",
        "int main(void){\n",
        "\n",
        "\tconst int TRAINING_SIZE = 4;\n",
        "\tconst int TRAINING_DIM = 4;\n",
        "\tconst int L1_SIZE = 8;\n",
        "\n",
        "\t// X, the first 4 lines from Iris dataset\n",
        "\tfloat h_X[TRAINING_SIZE*TRAINING_DIM] = {\t5.1, 3.5, 1.4, 0.2,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t4.9, 3.0, 1.4, 0.2,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t6.2, 3.4, 5.4, 2.3,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t5.9, 3.0, 5.1, 1.8 };\n",
        "\n",
        "\tconst signed int X_size = sizeof(h_X);\n",
        "\n",
        "\tfloat *d_X;\n",
        "\tcudaMalloc(&d_X, X_size);\n",
        "\tcudaMemcpy(d_X, h_X, X_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\t//WEIGHTS_0\n",
        "\tconst long signed int W0_size = L1_SIZE*TRAINING_DIM*sizeof(float);\n",
        "\tfloat *h_W0 = (float*)malloc(W0_size);\n",
        "\tfor (int i = 0; i < L1_SIZE*TRAINING_DIM; i++){\n",
        "\t    h_W0[i] = 0.1 * (2.0*rand()/RAND_MAX-1.0);\n",
        "\t}\n",
        "\n",
        "\tfloat *d_W0;\n",
        "\tcudaMalloc(&d_W0, W0_size);\n",
        "\tcudaMemcpy(d_W0, h_W0, W0_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\t//LAYER_1, LAYER_1_DELTA AND BUFFER OF LAYER 1 SIZE\n",
        "\tconst long signed int L1_size = L1_SIZE*TRAINING_SIZE*sizeof(float);\n",
        "\n",
        "\tfloat* h_layer_1 = (float*)malloc(L1_size);\n",
        "\tfloat* h_layer_1_delta = (float*)malloc(L1_size);\n",
        "\tfloat* h_buffer = (float*)malloc(L1_size);\n",
        "\n",
        "\tfor (int i = 0; i < L1_SIZE*TRAINING_SIZE; i++){\n",
        "\t    h_layer_1[i] = 0.0;\n",
        "\t    h_buffer[i] = 0.0;\n",
        "\t    h_layer_1_delta[i] = 0.0;\n",
        "\t}\n",
        "\n",
        "\tfloat *d_layer_1;\n",
        "\tcudaMalloc(&d_layer_1, L1_size);\n",
        "\tcudaMemcpy(d_layer_1, h_layer_1, L1_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\tfloat *d_buffer;\n",
        "\tcudaMalloc(&d_buffer, L1_size);\n",
        "\tcudaMemcpy(d_buffer, h_buffer, L1_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\tfloat *d_layer_1_delta;\n",
        "\tcudaMalloc(&d_layer_1_delta, L1_size);\n",
        "\tcudaMemcpy(d_layer_1_delta, h_layer_1_delta, L1_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\t//WEIGHTS_1\n",
        "\tconst long signed int W1_size = L1_SIZE*sizeof(float);\n",
        "\tfloat *h_W1 = (float*)malloc(W1_size);\n",
        "\tfor (int i = 0; i < L1_SIZE; i++){\n",
        "\t    h_W1[i] = 0.1* (2.0*rand()/RAND_MAX-1.0);\n",
        "\t}\n",
        "\n",
        "\tfloat *d_W1;\n",
        "\tcudaMalloc(&d_W1, W1_size);\n",
        "\tcudaMemcpy(d_W1, h_W1, W1_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\t//Y\n",
        "\tfloat h_y[4] = {\t0,\n",
        "\t\t\t\t\t\t0,\n",
        "\t\t\t\t\t\t1,\n",
        "\t\t\t\t\t\t1 };\n",
        "\tconst signed int y_size = sizeof(h_y);\n",
        "\tfloat *d_y;\n",
        "\tcudaMalloc(&d_y, y_size);\n",
        "\tcudaMemcpy(d_y, h_y, y_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\t//PRED AND PRED_DELTA\n",
        "\tfloat* h_pred = (float*)malloc(y_size);\n",
        "\tfloat* h_pred_delta = (float*)malloc(y_size);\n",
        "\tfor (int i = 0; i < TRAINING_SIZE; i++){\n",
        "\t    h_pred[i] = 0.0;\n",
        "\t    h_pred_delta[i] = 0.0;\n",
        "\t}\n",
        "\n",
        "\tfloat *d_pred;\n",
        "\tcudaMalloc(&d_pred, y_size);\n",
        "\tcudaMemcpy(d_pred, h_pred, y_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\tfloat *d_pred_delta;\n",
        "\tcudaMalloc(&d_pred_delta, y_size);\n",
        "\tcudaMemcpy(d_pred_delta, h_pred_delta, y_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\tkFit <<< 1, 1 >>> (\td_X, TRAINING_DIM, TRAINING_SIZE,\n",
        "\t\t\t\t\t\td_y, 1,\n",
        "\t\t\t\t\t\td_layer_1, L1_SIZE, d_layer_1_delta,\n",
        "\t\t\t\t\t\td_pred,\n",
        "\t\t\t\t\t\td_pred_delta,\n",
        "\t\t\t\t\t\td_W0,\n",
        "\t\t\t\t\t\td_W1,\n",
        "\t\t\t\t\t\td_buffer);\n",
        "\n",
        "\tcudaMemcpy(h_pred, d_pred, y_size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "\tcudaFree(d_pred);\n",
        "\tcudaFree(d_X);\n",
        "\tcudaFree(d_y);\n",
        "\tcudaFree(d_layer_1_delta);\n",
        "\tcudaFree(d_pred_delta);\n",
        "\tcudaFree(d_W0);\n",
        "\tcudaFree(d_W1);\n",
        "\tcudaFree(d_buffer);\n",
        "\n",
        "\tfree(h_layer_1_delta);\n",
        "\tfree(h_pred_delta);\n",
        "\tfree(h_W0);\n",
        "\tfree(h_W1);\n",
        "\tfree(h_buffer);\n",
        "\n",
        "\tfor (int i = 0; i < TRAINING_SIZE; i++){\n",
        "\t\tprintf(\"Prediction[%i] : %f True Value[%i] : %f Error[%i] : %f\\n\", i, h_pred[i], i, h_y[i], i, h_pred[i] - h_y[i]);\n",
        "\t}\n",
        "\n",
        "\tfree(h_pred);\n",
        "}\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/tmpnoyd3olk/351139b5-6680-4d40-8e92-3f5460a96e8d.cu(32): error: kernel launch from __device__ or __global__ functions requires separate compilation mode\n",
            "\n",
            "/tmp/tmpnoyd3olk/351139b5-6680-4d40-8e92-3f5460a96e8d.cu(33): warning #1444-D: function \"cudaDeviceSynchronize\"\n",
            "/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_device_runtime_api.h(139): here was declared deprecated (\"Use of cudaDeviceSynchronize from device code is deprecated. Moreover, such use will cause this module to fail to load on sm_90+ devices. If calls to cudaDeviceSynchronize from device code cannot be removed for older devices at this time, you may guard them with __CUDA_ARCH__ macros to remove them only for sm_90+ devices, making sure to generate code for compute_90 for the macros to take effect. Note that this mitigation will no longer work when support for cudaDeviceSynchronize from device code is eventually dropped for all devices. Disable this warning with -D__CDPRT_SUPPRESS_SYNC_DEPRECATION_WARNING.\")\n",
            "\n",
            "/tmp/tmpnoyd3olk/351139b5-6680-4d40-8e92-3f5460a96e8d.cu(55): error: kernel launch from __device__ or __global__ functions requires separate compilation mode\n",
            "\n",
            "/tmp/tmpnoyd3olk/351139b5-6680-4d40-8e92-3f5460a96e8d.cu(56): warning #1444-D: function \"cudaDeviceSynchronize\"\n",
            "/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_device_runtime_api.h(139): here was declared deprecated (\"Use of cudaDeviceSynchronize from device code is deprecated. Moreover, such use will cause this module to fail to load on sm_90+ devices. If calls to cudaDeviceSynchronize from device code cannot be removed for older devices at this time, you may guard them with __CUDA_ARCH__ macros to remove them only for sm_90+ devices, making sure to generate code for compute_90 for the macros to take effect. Note that this mitigation will no longer work when support for cudaDeviceSynchronize from device code is eventually dropped for all devices. Disable this warning with -D__CDPRT_SUPPRESS_SYNC_DEPRECATION_WARNING.\")\n",
            "\n",
            "/tmp/tmpnoyd3olk/351139b5-6680-4d40-8e92-3f5460a96e8d.cu(77): error: kernel launch from __device__ or __global__ functions requires separate compilation mode\n",
            "\n",
            "/tmp/tmpnoyd3olk/351139b5-6680-4d40-8e92-3f5460a96e8d.cu(78): warning #1444-D: function \"cudaDeviceSynchronize\"\n",
            "/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_device_runtime_api.h(139): here was declared deprecated (\"Use of cudaDeviceSynchronize from device code is deprecated. Moreover, such use will cause this module to fail to load on sm_90+ devices. If calls to cudaDeviceSynchronize from device code cannot be removed for older devices at this time, you may guard them with __CUDA_ARCH__ macros to remove them only for sm_90+ devices, making sure to generate code for compute_90 for the macros to take effect. Note that this mitigation will no longer work when support for cudaDeviceSynchronize from device code is eventually dropped for all devices. Disable this warning with -D__CDPRT_SUPPRESS_SYNC_DEPRECATION_WARNING.\")\n",
            "\n",
            "/tmp/tmpnoyd3olk/351139b5-6680-4d40-8e92-3f5460a96e8d.cu(99): error: kernel launch from __device__ or __global__ functions requires separate compilation mode\n",
            "\n",
            "/tmp/tmpnoyd3olk/351139b5-6680-4d40-8e92-3f5460a96e8d.cu(100): warning #1444-D: function \"cudaDeviceSynchronize\"\n",
            "/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_device_runtime_api.h(139): here was declared deprecated (\"Use of cudaDeviceSynchronize from device code is deprecated. Moreover, such use will cause this module to fail to load on sm_90+ devices. If calls to cudaDeviceSynchronize from device code cannot be removed for older devices at this time, you may guard them with __CUDA_ARCH__ macros to remove them only for sm_90+ devices, making sure to generate code for compute_90 for the macros to take effect. Note that this mitigation will no longer work when support for cudaDeviceSynchronize from device code is eventually dropped for all devices. Disable this warning with -D__CDPRT_SUPPRESS_SYNC_DEPRECATION_WARNING.\")\n",
            "\n",
            "/tmp/tmpnoyd3olk/351139b5-6680-4d40-8e92-3f5460a96e8d.cu(135): error: kernel launch from __device__ or __global__ functions requires separate compilation mode\n",
            "\n",
            "/tmp/tmpnoyd3olk/351139b5-6680-4d40-8e92-3f5460a96e8d.cu(136): warning #1444-D: function \"cudaDeviceSynchronize\"\n",
            "/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_device_runtime_api.h(139): here was declared deprecated (\"Use of cudaDeviceSynchronize from device code is deprecated. Moreover, such use will cause this module to fail to load on sm_90+ devices. If calls to cudaDeviceSynchronize from device code cannot be removed for older devices at this time, you may guard them with __CUDA_ARCH__ macros to remove them only for sm_90+ devices, making sure to generate code for compute_90 for the macros to take effect. Note that this mitigation will no longer work when support for cudaDeviceSynchronize from device code is eventually dropped for all devices. Disable this warning with -D__CDPRT_SUPPRESS_SYNC_DEPRECATION_WARNING.\")\n",
            "\n",
            "/tmp/tmpnoyd3olk/351139b5-6680-4d40-8e92-3f5460a96e8d.cu(171): error: kernel launch from __device__ or __global__ functions requires separate compilation mode\n",
            "\n",
            "/tmp/tmpnoyd3olk/351139b5-6680-4d40-8e92-3f5460a96e8d.cu(172): warning #1444-D: function \"cudaDeviceSynchronize\"\n",
            "/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_device_runtime_api.h(139): here was declared deprecated (\"Use of cudaDeviceSynchronize from device code is deprecated. Moreover, such use will cause this module to fail to load on sm_90+ devices. If calls to cudaDeviceSynchronize from device code cannot be removed for older devices at this time, you may guard them with __CUDA_ARCH__ macros to remove them only for sm_90+ devices, making sure to generate code for compute_90 for the macros to take effect. Note that this mitigation will no longer work when support for cudaDeviceSynchronize from device code is eventually dropped for all devices. Disable this warning with -D__CDPRT_SUPPRESS_SYNC_DEPRECATION_WARNING.\")\n",
            "\n",
            "/tmp/tmpnoyd3olk/351139b5-6680-4d40-8e92-3f5460a96e8d.cu(209): error: kernel launch from __device__ or __global__ functions requires separate compilation mode\n",
            "\n",
            "/tmp/tmpnoyd3olk/351139b5-6680-4d40-8e92-3f5460a96e8d.cu(210): warning #1444-D: function \"cudaDeviceSynchronize\"\n",
            "/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_device_runtime_api.h(139): here was declared deprecated (\"Use of cudaDeviceSynchronize from device code is deprecated. Moreover, such use will cause this module to fail to load on sm_90+ devices. If calls to cudaDeviceSynchronize from device code cannot be removed for older devices at this time, you may guard them with __CUDA_ARCH__ macros to remove them only for sm_90+ devices, making sure to generate code for compute_90 for the macros to take effect. Note that this mitigation will no longer work when support for cudaDeviceSynchronize from device code is eventually dropped for all devices. Disable this warning with -D__CDPRT_SUPPRESS_SYNC_DEPRECATION_WARNING.\")\n",
            "\n",
            "7 errors detected in the compilation of \"/tmp/tmpnoyd3olk/351139b5-6680-4d40-8e92-3f5460a96e8d.cu\".\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HhzhFEDtwUnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoxIGcAfoFBN"
      },
      "source": [
        "## Why Learn CUDA?\n",
        "Have a look https://github.com/pytorch/pytorch/tree/5bc44fb6ea65c711c818ea82dc66afee9ad48f78/aten/src/ATen/native/cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie3pMAGUsqaM"
      },
      "source": [
        "## Install CUDA in Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvvigQF_spPG"
      },
      "source": [
        "!apt update -qq;\n",
        "!wget https://developer.nvidia.com/compute/cuda/8.0/Prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb;\n",
        "!dpkg -i cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb;\n",
        "!apt-key add /var/cuda-repo-8-0-local-ga2/7fa2af80.pub;\n",
        "!apt-get update -qq;\n",
        "!apt-get install cuda gcc-5 g++-5 -y -qq;\n",
        "!ln -s /usr/bin/gcc-5 /usr/local/cuda/bin/gcc;\n",
        "!ln -s /usr/bin/g++-5 /usr/local/cuda/bin/g++;\n",
        "!apt install cuda-8.0;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef0oNYUWstr4",
        "outputId": "ea6204a6-bd29-4b00-f14c-5b055bbddf15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin\n",
        "\n",
        "\n",
        "%%cu\n",
        "#include <iostream>\n",
        "int main() {\n",
        "    std::cout << \"Hello world\\n\";\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-162d861j\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-162d861j\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp36-none-any.whl size=4307 sha256=801bf6cbe18412ebc8691bcd3172a7fb9c1d6c1b30193705fd2c8c7e1e2f2427\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cdlovexn/wheels/10/c2/05/ca241da37bff77d60d31a9174f988109c61ba989e4d4650516\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}